{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumy Algorithms\n",
    "\n",
    "* Using Latent Semantic Analysis\n",
    "* Using LexRank approach\n",
    "* Using TextRank approach\n",
    "* Using SumBasic approach\n",
    "* Using KL-sum\n",
    "\n",
    "### Requirements\n",
    "\n",
    "* fuzzy wuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fuzzywuzzy[speedup]\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "import numpy as np\n",
    "from utils import load_espn, save_espn\n",
    "path = r'C:/Users/atenk/Documents/ISM/HeadlineGeneration/ESPN_football_2.csv'\n",
    "df = load_espn(path)\n",
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>class</th>\n",
       "      <th>data-id</th>\n",
       "      <th>sport</th>\n",
       "      <th>teamname</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>summary</th>\n",
       "      <th>headline</th>\n",
       "      <th>rawText</th>\n",
       "      <th>...</th>\n",
       "      <th>LSASummarizer</th>\n",
       "      <th>LexRankSummarizer</th>\n",
       "      <th>TextRankSummarizer</th>\n",
       "      <th>SumBasicSummarizer</th>\n",
       "      <th>KLSumSummarizer</th>\n",
       "      <th>base_url</th>\n",
       "      <th>LSAFuzzy</th>\n",
       "      <th>LexRankFuzzy</th>\n",
       "      <th>TextRankFuzzy</th>\n",
       "      <th>SumBasicFuzzy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mike Rodak</td>\n",
       "      <td>story-link</td>\n",
       "      <td>buffalo-bills-33083</td>\n",
       "      <td>nfl</td>\n",
       "      <td>buffalo-bills</td>\n",
       "      <td>3d</td>\n",
       "      <td>http://espn.com/blog/buffalo-bills/post/_/id/3...</td>\n",
       "      <td>Andre Reed, who ranks seventh in NFL history w...</td>\n",
       "      <td>Andre Reed has acting bug after cameos in 'Mac...</td>\n",
       "      <td>Andre Reed, who ranks seventh in NFL history w...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>After a successful spot on the show in January...</td>\n",
       "      <td>11</td>\n",
       "      <td>\"Everything is timed,\" Reed said.</td>\n",
       "      <td>9</td>\n",
       "      <td>http://espn.com/blog</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Todd McShay</td>\n",
       "      <td>story-link</td>\n",
       "      <td>26489910</td>\n",
       "      <td>nfl</td>\n",
       "      <td>buffalo-bills</td>\n",
       "      <td>4d</td>\n",
       "      <td>http://insider.espn.com/nfl/draft2019/insider/...</td>\n",
       "      <td>Todd McShay hears that the Giants might not be...</td>\n",
       "      <td>Todd McShay's top five 2019 NFL draft needs fo...</td>\n",
       "      <td>Todd McShay hears that the Giants might not be...</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>The Bills have their quarterback in Josh Allen...</td>\n",
       "      <td>Top draft needs: TE, WR, DT, EDGE, QB</td>\n",
       "      <td>Top draft needs: QB, G, OT, C, EDGE</td>\n",
       "      <td>Top draft needs: TE, WR, DT, EDGE, QB</td>\n",
       "      <td>http://insider.espn.com/nfl</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ESPN.com</td>\n",
       "      <td>story-link</td>\n",
       "      <td>26473482</td>\n",
       "      <td>nfl</td>\n",
       "      <td>buffalo-bills</td>\n",
       "      <td>6d</td>\n",
       "      <td>http://www.espn.com/nfl/draft2019/story/_/id/2...</td>\n",
       "      <td>The NFL draft annually brings back memories, a...</td>\n",
       "      <td>NFL draft do-overs: Let's re-pick ... and fix ...</td>\n",
       "      <td>The NFL draft annually brings back memories, a...</td>\n",
       "      <td>...</td>\n",
       "      <td>Sure, just about every team in the NFL would l...</td>\n",
       "      <td>10 pick)</td>\n",
       "      <td>53</td>\n",
       "      <td>10 pick)</td>\n",
       "      <td>First round of the 2016 draft (21st overall)Ac...</td>\n",
       "      <td>http://www.espn.com/nfl</td>\n",
       "      <td>145</td>\n",
       "      <td>65</td>\n",
       "      <td>53</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T.J. Berka</td>\n",
       "      <td>story-link</td>\n",
       "      <td>24367000</td>\n",
       "      <td>nfl</td>\n",
       "      <td>buffalo-bills</td>\n",
       "      <td>7d</td>\n",
       "      <td>http://www.espn.com/nhl/story/_/id/24367000/th...</td>\n",
       "      <td>Since we updated the sports misery index in De...</td>\n",
       "      <td>Sports misery index: Most miserable fan bases ...</td>\n",
       "      <td>Since we updated the sports misery index in De...</td>\n",
       "      <td>...</td>\n",
       "      <td>The Brewers were the focus of this article whe...</td>\n",
       "      <td>189</td>\n",
       "      <td>130</td>\n",
       "      <td>14</td>\n",
       "      <td>96</td>\n",
       "      <td>http://www.espn.com/nhl</td>\n",
       "      <td>150</td>\n",
       "      <td>189</td>\n",
       "      <td>130</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mike Rodak</td>\n",
       "      <td>story-link</td>\n",
       "      <td>26426098</td>\n",
       "      <td>nfl</td>\n",
       "      <td>buffalo-bills</td>\n",
       "      <td>13d</td>\n",
       "      <td>http://www.espn.com/nfl/story/_/id/26426098/in...</td>\n",
       "      <td>Former NFL offensive lineman Richie Incognito ...</td>\n",
       "      <td>Incognito pleads guilty to disorderly conduct</td>\n",
       "      <td>Former NFL offensive lineman Richie Incognito ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Appearing on a podcast hosted by former Bills ...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Appearing on a podcast hosted by former Bills ...</td>\n",
       "      <td>http://www.espn.com/nfl</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        author       class              data-id sport       teamname  \\\n",
       "0   Mike Rodak  story-link  buffalo-bills-33083   nfl  buffalo-bills   \n",
       "1  Todd McShay  story-link             26489910   nfl  buffalo-bills   \n",
       "2     ESPN.com  story-link             26473482   nfl  buffalo-bills   \n",
       "3   T.J. Berka  story-link             24367000   nfl  buffalo-bills   \n",
       "4   Mike Rodak  story-link             26426098   nfl  buffalo-bills   \n",
       "\n",
       "  timestamp                                                url  \\\n",
       "0        3d  http://espn.com/blog/buffalo-bills/post/_/id/3...   \n",
       "1        4d  http://insider.espn.com/nfl/draft2019/insider/...   \n",
       "2        6d  http://www.espn.com/nfl/draft2019/story/_/id/2...   \n",
       "3        7d  http://www.espn.com/nhl/story/_/id/24367000/th...   \n",
       "4       13d  http://www.espn.com/nfl/story/_/id/26426098/in...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Andre Reed, who ranks seventh in NFL history w...   \n",
       "1  Todd McShay hears that the Giants might not be...   \n",
       "2  The NFL draft annually brings back memories, a...   \n",
       "3  Since we updated the sports misery index in De...   \n",
       "4  Former NFL offensive lineman Richie Incognito ...   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Andre Reed has acting bug after cameos in 'Mac...   \n",
       "1  Todd McShay's top five 2019 NFL draft needs fo...   \n",
       "2  NFL draft do-overs: Let's re-pick ... and fix ...   \n",
       "3  Sports misery index: Most miserable fan bases ...   \n",
       "4      Incognito pleads guilty to disorderly conduct   \n",
       "\n",
       "                                             rawText      ...       \\\n",
       "0  Andre Reed, who ranks seventh in NFL history w...      ...        \n",
       "1  Todd McShay hears that the Giants might not be...      ...        \n",
       "2  The NFL draft annually brings back memories, a...      ...        \n",
       "3  Since we updated the sports misery index in De...      ...        \n",
       "4  Former NFL offensive lineman Richie Incognito ...      ...        \n",
       "\n",
       "                                       LSASummarizer  \\\n",
       "0                                                  0   \n",
       "1                                                  6   \n",
       "2  Sure, just about every team in the NFL would l...   \n",
       "3  The Brewers were the focus of this article whe...   \n",
       "4  Appearing on a podcast hosted by former Bills ...   \n",
       "\n",
       "                                   LexRankSummarizer  \\\n",
       "0  After a successful spot on the show in January...   \n",
       "1  The Bills have their quarterback in Josh Allen...   \n",
       "2                                           10 pick)   \n",
       "3                                                189   \n",
       "4                                                 12   \n",
       "\n",
       "                      TextRankSummarizer                   SumBasicSummarizer  \\\n",
       "0                                     11    \"Everything is timed,\" Reed said.   \n",
       "1  Top draft needs: TE, WR, DT, EDGE, QB  Top draft needs: QB, G, OT, C, EDGE   \n",
       "2                                     53                             10 pick)   \n",
       "3                                    130                                   14   \n",
       "4                                     12                                   12   \n",
       "\n",
       "                                     KLSumSummarizer  \\\n",
       "0                                                  9   \n",
       "1              Top draft needs: TE, WR, DT, EDGE, QB   \n",
       "2  First round of the 2016 draft (21st overall)Ac...   \n",
       "3                                                 96   \n",
       "4  Appearing on a podcast hosted by former Bills ...   \n",
       "\n",
       "                      base_url LSAFuzzy LexRankFuzzy TextRankFuzzy  \\\n",
       "0         http://espn.com/blog       17            5            11   \n",
       "1  http://insider.espn.com/nfl        6            5            14   \n",
       "2      http://www.espn.com/nfl      145           65            53   \n",
       "3      http://www.espn.com/nhl      150          189           130   \n",
       "4      http://www.espn.com/nfl        6           12            12   \n",
       "\n",
       "  SumBasicFuzzy  \n",
       "0            14  \n",
       "1             0  \n",
       "2            65  \n",
       "3            14  \n",
       "4            12  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_summarize (url, text):\n",
    "    from sumy.parsers.html import HtmlParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer\n",
    "    from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "    from sumy.nlp.stemmers import Stemmer\n",
    "    from sumy.utils import get_stop_words\n",
    "    \n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 1\n",
    "\n",
    "    parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    sent = \"\";\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        sent += str(sentence)\n",
    "    try:\n",
    "        return text.index(sent)\n",
    "    except:\n",
    "        return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa(df):\n",
    "    df['LSASummarizer'] = lsa_summarize(df['url'], df['text'])\n",
    "    return df\n",
    "df = df.apply(lsa, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_fuzzy (url, text):\n",
    "    from sumy.parsers.html import HtmlParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer\n",
    "    from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "    from sumy.nlp.stemmers import Stemmer\n",
    "    from sumy.utils import get_stop_words\n",
    "    \n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 1\n",
    "\n",
    "    parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    sent = \"\";\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        sent += str(sentence)\n",
    "    try:\n",
    "        return text.index(sent)\n",
    "    except:\n",
    "        return text.index(process.extract(sent, text)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsaf(df):\n",
    "    df['LSAFuzzy'] = lsa_fuzzy(df['url'], df['text'])\n",
    "    return df\n",
    "df = df.apply(lsaf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexrank_summarize (url, text):\n",
    "    from sumy.parsers.html import HtmlParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer\n",
    "    from sumy.summarizers.lex_rank import LexRankSummarizer as Summarizer\n",
    "    from sumy.nlp.stemmers import Stemmer\n",
    "    from sumy.utils import get_stop_words\n",
    "\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 1\n",
    "\n",
    "    parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    sent = \"\";\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        sent += str(sentence)\n",
    "    try:\n",
    "        return text.index(sent)\n",
    "    except:\n",
    "        return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrs(df):\n",
    "    df['LexRankSummarizer'] = lexrank_summarize(df['url'], df['text'])\n",
    "    return df\n",
    "df = df.apply(lrs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexrank_fuzzy (url, text):\n",
    "    from sumy.parsers.html import HtmlParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer\n",
    "    from sumy.summarizers.lex_rank import LexRankSummarizer as Summarizer\n",
    "    from sumy.nlp.stemmers import Stemmer\n",
    "    from sumy.utils import get_stop_words\n",
    "\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 1\n",
    "\n",
    "    parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    sent = \"\";\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        sent += str(sentence)\n",
    "    try:\n",
    "        return text.index(sent)\n",
    "    except:\n",
    "        return text.index(process.extract(sent, text)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrsf(df):\n",
    "    df['LexRankFuzzy'] = lexrank_fuzzy(df['url'], df['text'])\n",
    "    return df\n",
    "df = df.apply(lrsf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_summarize (url, text):\n",
    "    from sumy.parsers.html import HtmlParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer\n",
    "    from sumy.summarizers.text_rank import TextRankSummarizer as Summarizer\n",
    "    from sumy.nlp.stemmers import Stemmer\n",
    "    from sumy.utils import get_stop_words\n",
    "\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 1\n",
    "\n",
    "    parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    sent = \"\";\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        sent += str(sentence)\n",
    "    try:\n",
    "        return text.index(sent)\n",
    "    except:\n",
    "        return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trs(df):\n",
    "    df['TextRankSummarizer'] = textrank_summarize(df['url'], df['text'])\n",
    "    return df\n",
    "df = df.apply(trs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_fuzzy (url, text):\n",
    "    from sumy.parsers.html import HtmlParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer\n",
    "    from sumy.summarizers.text_rank import TextRankSummarizer as Summarizer\n",
    "    from sumy.nlp.stemmers import Stemmer\n",
    "    from sumy.utils import get_stop_words\n",
    "\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 1\n",
    "\n",
    "    parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    sent = \"\";\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        sent += str(sentence)\n",
    "    try:\n",
    "        return text.index(sent)\n",
    "    except:\n",
    "        return text.index(process.extract(sent, text)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trsf(df):\n",
    "    df['TextRankFuzzy'] = textrank_fuzzy(df['url'], df['text'])\n",
    "    return df\n",
    "df = df.apply(trsf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumbasic_summarize (url, text):\n",
    "    from sumy.parsers.html import HtmlParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer\n",
    "    from sumy.summarizers.sum_basic import SumBasicSummarizer as Summarizer\n",
    "    from sumy.nlp.stemmers import Stemmer\n",
    "    from sumy.utils import get_stop_words\n",
    "\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 1\n",
    "\n",
    "    parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    sent = \"\";\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        sent += str(sentence)\n",
    "    try:\n",
    "        return text.index(sent)\n",
    "    except:\n",
    "        return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbs(df):\n",
    "    df['SumBasicSummarizer'] = sumbasic_summarize(df['url'], df['text'])\n",
    "    return df\n",
    "df = df.apply(sbs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumbasic_fuzzy (url, text):\n",
    "    from sumy.parsers.html import HtmlParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer\n",
    "    from sumy.summarizers.sum_basic import SumBasicSummarizer as Summarizer\n",
    "    from sumy.nlp.stemmers import Stemmer\n",
    "    from sumy.utils import get_stop_words\n",
    "\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 1\n",
    "\n",
    "    parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    sent = \"\";\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        sent += str(sentence)\n",
    "    try:\n",
    "        return text.index(sent)\n",
    "    except:\n",
    "        return text.index(process.extract(sent, text)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbsf(df):\n",
    "    df['SumBasicFuzzy'] = sumbasic_fuzzy(df['url'], df['text'])\n",
    "    return df\n",
    "df = df.apply(sbsf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def klsum_summarize (url, text):\n",
    "    from sumy.parsers.html import HtmlParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer\n",
    "    from sumy.summarizers.kl import KLSummarizer as Summarizer\n",
    "    from sumy.nlp.stemmers import Stemmer\n",
    "    from sumy.utils import get_stop_words\n",
    "\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 1\n",
    "\n",
    "    parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    sent = \"\";\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        sent += str(sentence)\n",
    "    try:\n",
    "        return text.index(sent)\n",
    "    except:\n",
    "        return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kls(df):\n",
    "    df['KLSumSummarizer'] = klsum_summarize(df['url'], df['text'])\n",
    "    return df\n",
    "df = df.apply(kls, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def klsum_fuzzy (text, KLSummary):\n",
    "    if KLSummary.isdigit():\n",
    "        return KLSummary\n",
    "    else:\n",
    "        return text.index(process.extract(KLSummary, text)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def klsf(df):\n",
    "    df['KLSumFuzzy'] = klsum_fuzzy(df['text'], df['KLSumSummarizer'])\n",
    "    return df\n",
    "df = df.apply(klsf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>class</th>\n",
       "      <th>data-id</th>\n",
       "      <th>sport</th>\n",
       "      <th>teamname</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>summary</th>\n",
       "      <th>headline</th>\n",
       "      <th>rawText</th>\n",
       "      <th>...</th>\n",
       "      <th>LexRankSummarizer</th>\n",
       "      <th>TextRankSummarizer</th>\n",
       "      <th>SumBasicSummarizer</th>\n",
       "      <th>KLSumSummarizer</th>\n",
       "      <th>base_url</th>\n",
       "      <th>LSAFuzzy</th>\n",
       "      <th>LexRankFuzzy</th>\n",
       "      <th>TextRankFuzzy</th>\n",
       "      <th>SumBasicFuzzy</th>\n",
       "      <th>KLSumFuzzy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mike Rodak</td>\n",
       "      <td>story-link</td>\n",
       "      <td>buffalo-bills-33083</td>\n",
       "      <td>nfl</td>\n",
       "      <td>buffalo-bills</td>\n",
       "      <td>3d</td>\n",
       "      <td>http://espn.com/blog/buffalo-bills/post/_/id/3...</td>\n",
       "      <td>Andre Reed, who ranks seventh in NFL history w...</td>\n",
       "      <td>Andre Reed has acting bug after cameos in 'Mac...</td>\n",
       "      <td>Andre Reed, who ranks seventh in NFL history w...</td>\n",
       "      <td>...</td>\n",
       "      <td>After a successful spot on the show in January...</td>\n",
       "      <td>11</td>\n",
       "      <td>\"Everything is timed,\" Reed said.</td>\n",
       "      <td>9</td>\n",
       "      <td>http://espn.com/blog</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Todd McShay</td>\n",
       "      <td>story-link</td>\n",
       "      <td>26489910</td>\n",
       "      <td>nfl</td>\n",
       "      <td>buffalo-bills</td>\n",
       "      <td>4d</td>\n",
       "      <td>http://insider.espn.com/nfl/draft2019/insider/...</td>\n",
       "      <td>Todd McShay hears that the Giants might not be...</td>\n",
       "      <td>Todd McShay's top five 2019 NFL draft needs fo...</td>\n",
       "      <td>Todd McShay hears that the Giants might not be...</td>\n",
       "      <td>...</td>\n",
       "      <td>The Bills have their quarterback in Josh Allen...</td>\n",
       "      <td>Top draft needs: TE, WR, DT, EDGE, QB</td>\n",
       "      <td>Top draft needs: QB, G, OT, C, EDGE</td>\n",
       "      <td>Top draft needs: TE, WR, DT, EDGE, QB</td>\n",
       "      <td>http://insider.espn.com/nfl</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ESPN.com</td>\n",
       "      <td>story-link</td>\n",
       "      <td>26473482</td>\n",
       "      <td>nfl</td>\n",
       "      <td>buffalo-bills</td>\n",
       "      <td>6d</td>\n",
       "      <td>http://www.espn.com/nfl/draft2019/story/_/id/2...</td>\n",
       "      <td>The NFL draft annually brings back memories, a...</td>\n",
       "      <td>NFL draft do-overs: Let's re-pick ... and fix ...</td>\n",
       "      <td>The NFL draft annually brings back memories, a...</td>\n",
       "      <td>...</td>\n",
       "      <td>10 pick)</td>\n",
       "      <td>53</td>\n",
       "      <td>10 pick)</td>\n",
       "      <td>First round of the 2016 draft (21st overall)Ac...</td>\n",
       "      <td>http://www.espn.com/nfl</td>\n",
       "      <td>145</td>\n",
       "      <td>65</td>\n",
       "      <td>53</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T.J. Berka</td>\n",
       "      <td>story-link</td>\n",
       "      <td>24367000</td>\n",
       "      <td>nfl</td>\n",
       "      <td>buffalo-bills</td>\n",
       "      <td>7d</td>\n",
       "      <td>http://www.espn.com/nhl/story/_/id/24367000/th...</td>\n",
       "      <td>Since we updated the sports misery index in De...</td>\n",
       "      <td>Sports misery index: Most miserable fan bases ...</td>\n",
       "      <td>Since we updated the sports misery index in De...</td>\n",
       "      <td>...</td>\n",
       "      <td>189</td>\n",
       "      <td>130</td>\n",
       "      <td>14</td>\n",
       "      <td>96</td>\n",
       "      <td>http://www.espn.com/nhl</td>\n",
       "      <td>150</td>\n",
       "      <td>189</td>\n",
       "      <td>130</td>\n",
       "      <td>14</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mike Rodak</td>\n",
       "      <td>story-link</td>\n",
       "      <td>26426098</td>\n",
       "      <td>nfl</td>\n",
       "      <td>buffalo-bills</td>\n",
       "      <td>13d</td>\n",
       "      <td>http://www.espn.com/nfl/story/_/id/26426098/in...</td>\n",
       "      <td>Former NFL offensive lineman Richie Incognito ...</td>\n",
       "      <td>Incognito pleads guilty to disorderly conduct</td>\n",
       "      <td>Former NFL offensive lineman Richie Incognito ...</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Appearing on a podcast hosted by former Bills ...</td>\n",
       "      <td>http://www.espn.com/nfl</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        author       class              data-id sport       teamname  \\\n",
       "0   Mike Rodak  story-link  buffalo-bills-33083   nfl  buffalo-bills   \n",
       "1  Todd McShay  story-link             26489910   nfl  buffalo-bills   \n",
       "2     ESPN.com  story-link             26473482   nfl  buffalo-bills   \n",
       "3   T.J. Berka  story-link             24367000   nfl  buffalo-bills   \n",
       "4   Mike Rodak  story-link             26426098   nfl  buffalo-bills   \n",
       "\n",
       "  timestamp                                                url  \\\n",
       "0        3d  http://espn.com/blog/buffalo-bills/post/_/id/3...   \n",
       "1        4d  http://insider.espn.com/nfl/draft2019/insider/...   \n",
       "2        6d  http://www.espn.com/nfl/draft2019/story/_/id/2...   \n",
       "3        7d  http://www.espn.com/nhl/story/_/id/24367000/th...   \n",
       "4       13d  http://www.espn.com/nfl/story/_/id/26426098/in...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Andre Reed, who ranks seventh in NFL history w...   \n",
       "1  Todd McShay hears that the Giants might not be...   \n",
       "2  The NFL draft annually brings back memories, a...   \n",
       "3  Since we updated the sports misery index in De...   \n",
       "4  Former NFL offensive lineman Richie Incognito ...   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Andre Reed has acting bug after cameos in 'Mac...   \n",
       "1  Todd McShay's top five 2019 NFL draft needs fo...   \n",
       "2  NFL draft do-overs: Let's re-pick ... and fix ...   \n",
       "3  Sports misery index: Most miserable fan bases ...   \n",
       "4      Incognito pleads guilty to disorderly conduct   \n",
       "\n",
       "                                             rawText    ...      \\\n",
       "0  Andre Reed, who ranks seventh in NFL history w...    ...       \n",
       "1  Todd McShay hears that the Giants might not be...    ...       \n",
       "2  The NFL draft annually brings back memories, a...    ...       \n",
       "3  Since we updated the sports misery index in De...    ...       \n",
       "4  Former NFL offensive lineman Richie Incognito ...    ...       \n",
       "\n",
       "                                   LexRankSummarizer  \\\n",
       "0  After a successful spot on the show in January...   \n",
       "1  The Bills have their quarterback in Josh Allen...   \n",
       "2                                           10 pick)   \n",
       "3                                                189   \n",
       "4                                                 12   \n",
       "\n",
       "                      TextRankSummarizer                   SumBasicSummarizer  \\\n",
       "0                                     11    \"Everything is timed,\" Reed said.   \n",
       "1  Top draft needs: TE, WR, DT, EDGE, QB  Top draft needs: QB, G, OT, C, EDGE   \n",
       "2                                     53                             10 pick)   \n",
       "3                                    130                                   14   \n",
       "4                                     12                                   12   \n",
       "\n",
       "                                     KLSumSummarizer  \\\n",
       "0                                                  9   \n",
       "1              Top draft needs: TE, WR, DT, EDGE, QB   \n",
       "2  First round of the 2016 draft (21st overall)Ac...   \n",
       "3                                                 96   \n",
       "4  Appearing on a podcast hosted by former Bills ...   \n",
       "\n",
       "                      base_url  LSAFuzzy LexRankFuzzy TextRankFuzzy  \\\n",
       "0         http://espn.com/blog        17            5            11   \n",
       "1  http://insider.espn.com/nfl         6            5            14   \n",
       "2      http://www.espn.com/nfl       145           65            53   \n",
       "3      http://www.espn.com/nhl       150          189           130   \n",
       "4      http://www.espn.com/nfl         6           12            12   \n",
       "\n",
       "  SumBasicFuzzy KLSumFuzzy  \n",
       "0            14          9  \n",
       "1             0         14  \n",
       "2            65          5  \n",
       "3            14         96  \n",
       "4            12          6  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_espn(df, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
